{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "src_dir = os.path.join(os.getcwd(), 'src')\n",
    "sys.path.append(os.path.abspath(src_dir))\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import src.utils.data_loader\n",
    "\n",
    "reload(src.utils.data_loader)\n",
    "\n",
    "from src.utils.data_loader import DataEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_station_df = pd.read_csv('../data/liste-des-gares.csv', sep=';')\n",
    "\n",
    "departure_train_df = pd.read_csv('../data/dataset/departure_train.csv')\n",
    "departure_valid_df = pd.read_csv('../data/dataset/departure_valid.csv')\n",
    "\n",
    "arrival_train_df = pd.read_csv('../data/dataset/arrival_train.csv')\n",
    "arrival_valid_df = pd.read_csv('../data/dataset/arrival_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 30\n",
    "epochs = 2\n",
    "learning_rate = 2e-5\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\")\n",
    "bert_model = AutoModel.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\")\n",
    "max_len = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = departure_train_df\n",
    "valid_df = departure_valid_df\n",
    "number_of_training_sentences_per_station = -1\n",
    "number_of_training_sentences_per_station_valid = -1\n",
    "number_of_train_stations = departure_train_df['departure'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5339043475f24da99bdb7f005a3f3e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_train_df = pd.DataFrame()\n",
    "new_valid_df = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=number_of_train_stations) as pbar:\n",
    "    for i in range(number_of_train_stations):\n",
    "        if number_of_training_sentences_per_station != -1 :\n",
    "            train_df_subset = train_df[train_df[\"departure_train_station_id\"] == i]\n",
    "            new_train_df = pd.concat([new_train_df, train_df_subset])\n",
    "        \n",
    "        if number_of_training_sentences_per_station_valid != -1:\n",
    "            valid_df_subset = valid_df[valid_df[\"departure_train_station_id\"] == i].head(number_of_training_sentences_per_station_valid)\n",
    "            new_valid_df = pd.concat([new_valid_df, valid_df_subset])\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "if number_of_training_sentences_per_station != -1:\n",
    "    train_df = new_train_df.reset_index(drop=True)\n",
    "if number_of_training_sentences_per_station_valid != -1:\n",
    "    valid_df = new_valid_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_train_stations = LabelEncoder()\n",
    "\n",
    "le_train_stations.fit(train_station_df[\"LIBELLE\"].unique())\n",
    "\n",
    "# train_df['departure'] = le_train_stations.transform(train_df['departure'])\n",
    "# train_df['arrival'] = le_train_stations.transform(train_df['arrival'])\n",
    "\n",
    "# valid_df['departure'] = le_train_stations.transform(valid_df['departure'])\n",
    "# valid_df['arrival'] = le_train_stations.transform(valid_df['arrival'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataEncoder(\n",
    "    train_df[\"sentence\"], train_df[\"departure\"], train_df[\"arrival\"], tokenizer, max_len)\n",
    "val_dataset = DataEncoder(\n",
    "    valid_df[\"sentence\"], valid_df[\"departure\"], valid_df[\"arrival\"], tokenizer, max_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, n_depart, n_arrival):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out_depart = nn.Linear(self.bert.config.hidden_size, n_depart)\n",
    "        self.out_arrival = nn.Linear(self.bert.config.hidden_size, n_arrival)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        output_depart = self.out_depart(self.drop(pooled_output))\n",
    "        output_arrival = self.out_arrival(self.drop(pooled_output))\n",
    "\n",
    "        return output_depart, output_arrival\n",
    "\n",
    "    def freeze_arrival_layer(self):\n",
    "        for param in self.out_arrival.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_arrival_layer(self):\n",
    "        for param in self.out_arrival.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def freeze_depart_layer(self):\n",
    "        for param in self.out_depart.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_depart_layer(self):\n",
    "        for param in self.out_depart.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification(len(le_train_stations.classes_), len(le_train_stations.classes_))\n",
    "model.load_state_dict(torch.load(\"./processed/departure_arrival_model3_trained.pth\"))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "def train_epoch(model, data_loader_train, data_loader_valid, loss_fn, optimizer, device, current_epoch):\n",
    "    model = model.train()\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "    correct_predictions_train = 0\n",
    "    correct_predictions_valid = 0\n",
    "\n",
    "    with tqdm(total=len(data_loader_train), desc=f\"Epoch {current_epoch}\", unit=\"batch\") as pbar:\n",
    "        for d in data_loader_train:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels_depart = d[\"departure\"].to(device)\n",
    "            labels_arrival = d[\"arrival\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs_depart, outputs_arrival = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            loss_depart = loss_fn(outputs_depart, labels_depart)\n",
    "            loss_arrival = loss_fn(outputs_arrival, labels_arrival)\n",
    "            loss = loss_depart + loss_arrival\n",
    "\n",
    "            correct_predictions_train += (outputs_depart.argmax(1) == labels_depart).sum().item()\n",
    "            correct_predictions_train += (outputs_arrival.argmax(1) == labels_arrival).sum().item()\n",
    "\n",
    "            losses_train.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    for d in data_loader_valid:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels_depart = d[\"departure\"].to(device)\n",
    "        labels_arrival = d[\"arrival\"].to(device)\n",
    "\n",
    "        outputs_depart, outputs_arrival = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        loss_depart = loss_fn(outputs_depart, labels_depart)\n",
    "        loss_arrival = loss_fn(outputs_arrival, labels_arrival)\n",
    "        loss =  loss_depart + loss_arrival\n",
    "\n",
    "        correct_predictions_valid += (outputs_depart.argmax(1) == labels_depart).sum().item()\n",
    "        correct_predictions_valid += (outputs_arrival.argmax(1) == labels_arrival).sum().item()\n",
    "\n",
    "        losses_valid.append(loss.item())\n",
    "\n",
    "    train_acc = correct_predictions_train / (2 * len(data_loader_train.dataset))\n",
    "    train_loss = np.mean(losses_train)\n",
    "\n",
    "    valid_acc = correct_predictions_valid / (2 * len(data_loader_valid.dataset))\n",
    "    valid_loss = np.mean(losses_valid)\n",
    "\n",
    "    return {\"train_acc\": train_acc, \"train_loss\": train_loss, \"valid_acc\": valid_acc, \"valid_loss\": valid_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4aa3835cdc49f4b315f0efe14ee473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/17854 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danyl\\miniconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 4\u001b[0m     results \u001b[38;5;241m=\u001b[39m train_epoch(\n\u001b[0;32m      5\u001b[0m         model,\n\u001b[0;32m      6\u001b[0m         train_loader,\n\u001b[0;32m      7\u001b[0m         val_loader,\n\u001b[0;32m      8\u001b[0m         loss_fn,\n\u001b[0;32m      9\u001b[0m         optimizer,\n\u001b[0;32m     10\u001b[0m         device,\n\u001b[0;32m     11\u001b[0m         epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     12\u001b[0m     )\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[0;32m     14\u001b[0m     history\u001b[38;5;241m.\u001b[39mappend(results)\n",
      "Cell \u001b[1;32mIn[14], line 32\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, data_loader_train, data_loader_valid, loss_fn, optimizer, device, current_epoch)\u001b[0m\n\u001b[0;32m     29\u001b[0m loss_arrival \u001b[38;5;241m=\u001b[39m loss_fn(outputs_arrival, labels_arrival)\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_depart \u001b[38;5;241m+\u001b[39m loss_arrival\n\u001b[1;32m---> 32\u001b[0m correct_predictions_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (outputs_depart\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels_depart)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     33\u001b[0m correct_predictions_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (outputs_arrival\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels_arrival)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     35\u001b[0m losses_train\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    results = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        epoch + 1\n",
    "    )\n",
    "    print(results)\n",
    "    history.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./model/processed/departure_arrival_model3_trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(history)\n",
    "results_df.to_csv(\"./model/processed/model3_departure_arrival_training_history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vie à Allonnes-Boisville. Je vourais avoir un itinéraire de Châtenois (Bas-Rhin) à Thieux-Nantouillet en passant par Béziers\n",
      "Depart: Châtenois (Bas-Rhin), Arrival: Thieux-Nantouillet\n"
     ]
    }
   ],
   "source": [
    "sentence = f\"Je vie à {le_train_stations.classes_[50]}. Je vourais avoir un itinéraire de {le_train_stations.classes_[695]} à {le_train_stations.classes_[3102]} en passant par {le_train_stations.classes_[501]}\"\n",
    "# Tokenisez la phrase\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Récupérez les input_ids et attention_mask\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "print(sentence)\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "depart = le_train_stations.inverse_transform(torch.max(outputs[0], 1).indices)\n",
    "arrival = le_train_stations.inverse_transform(torch.max(outputs[1], 1).indices)\n",
    "print(f\"Depart: {depart[0]}, Arrival: {arrival[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
